{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "This part of the notebook aims to create a third-order letter approximation model using five English works in Plain Text UTF8 format from Project Gutenberg for my module Emerging Technologies in final year. The task is outlined as follows:\n",
    "\n",
    "Select five free English works in Plain Text UTF8 format from Project Gutenberg. Use them to create a model of the English language as follows. Remove any preamble and postamble. Remove all characters except for (ASCII) letters (uppercase and lowercase), full stops, and spaces. Make all letters uppercase.\n",
    "\n",
    "Next create a trigram model by counting the number of times each sequence of three characters (that is, each trigram) appears. You can design your own data structure for storing the results but explain your design and its rationale in your answer.\n",
    "\n",
    "For example, the sentence: It is what it is. would become IT IS WHAT IT IS. This will give a model like {'IT ': 2, 'T I': 3, ' IS': 2, 'IS ': 1, ...}.\n",
    "\n",
    "I have broken this down to the following steps:\n",
    "\n",
    "1. Selecting five texts from books I have gotten from https://www.gutenberg.org/.\n",
    "2. Cleaning the texts by removing preamble, postamble, and unnecessary characters.\n",
    "3. Converting the texts to uppercase.\n",
    "(Step 3 & 4 are to prepare the file for processing in the trigram model)\n",
    "4. Creating a trigram model by counting sequences of three consecutive characters.\n",
    "\n",
    "Trigrams are sequences of three characters, and the goal is to use trigrams to analyze the structure of the English language. Each trigram's frequency will be recorded to create a model for the English language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All imports used in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import json # for saving as json object notation in task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1.1\n",
    ">Reading the texts to files and storing texts to a lists 'texts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text files have been read in and stored to list 'texts' .\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read the selected text files\n",
    "\n",
    "# Defining the file paths for the uploaded text files of each book which are inside the \"T1Files\" folder for neatness sake.\n",
    "file_paths = [\n",
    "    \"T1Files/Hamlet.txt\",\n",
    "    \"T1Files/MacBeth.txt\",\n",
    "    \"T1Files/Romeo&Juliet.txt\",\n",
    "    \"T1Files/TheValley.txt\",\n",
    "    \"T1Files/TheVoiceOfTheVoid.txt\"\n",
    "]\n",
    "\n",
    "# Reading each file's content from the files_paths list and storing the content in a list\n",
    "texts = []\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        texts.append(file.read())\n",
    "\n",
    "\n",
    "print(\"Text files have been read in and stored to list 'texts' .\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1.2 Combining the texts to one file\n",
    ">Using the join method to join all texts into one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining text to one file. using join method to combine the text files to one file\n",
    "combined_text = \" \".join(texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1.3 Cleaned Text \n",
    "> Removing any preamble and postamble.<br>\n",
    "    Removing any characters that were not ASCII.<br>\n",
    "    Removing any characters that were not full stops or spaces.<br>\n",
    "    Made all characters uppercase.<br>\n",
    "    Combined all the text into one large string for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts have been cleaned and formatted.\n"
     ]
    }
   ],
   "source": [
    "def clean_and_format_text(text):\n",
    "    # Specifying start of text and end of text using regular expressions\n",
    "    start_marker = re.search(r\"\\*\\*\\* START OF .* \\*\\*\\*\", text)\n",
    "    if start_marker:\n",
    "        text = text[start_marker.end():]\n",
    "    \n",
    "    #  Removing any characters that were not ASCII & removing any characters that were not full stops or spaces\n",
    "    cleaned_text = re.sub(r\"[^a-zA-Z. ]\", \"\", text)\n",
    "    \n",
    "    # Made all characters uppercase\n",
    "    cleaned_text = cleaned_text.upper()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Pass our text and returning the cleaned and formatted text\n",
    "cleaned_texts = [clean_and_format_text(text) for text in texts]\n",
    "\n",
    "# Combines all of our cleaned texts into one large string for trigram analysis\n",
    "cleaned_combined_text = \" \".join(cleaned_texts)\n",
    "print(\"Texts have been cleaned and formatted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1.4 Create Trigram Model\n",
    ">Extracts trigrams. <br>\n",
    ">Keeps a count of the trigrams. <br>\n",
    ">Returns a sample of the trigrams extracted and their counts.<br>\n",
    "\n",
    ">A **trigram** is a sequence of three consecutive characters.<br>\n",
    " In this step of the task, we scan through the cleaned text and extract all possible trigrams. Each trigram is then stored in a dictionary, where the key is the trigram stated. The **value** is the number of times its counted in the text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram model created.\n",
      "\n",
      " Sample trigrams:\n",
      "{'PRO': 922, 'ROJ': 454, 'OJE': 454, 'JEC': 465, 'ECT': 962, 'CT ': 516, 'T G': 498, ' GU': 456, 'GUT': 470, 'UTE': 589}\n"
     ]
    }
   ],
   "source": [
    "def generate_trigrams(text):\n",
    "    trigram_counts = {}\n",
    "    \n",
    "    # Loop through the text and generate trigrams\n",
    "    for i in range(len(text) - 2): \n",
    "        trigram = text[i:i+3]\n",
    "        \n",
    "        # Increment count of trigrams\n",
    "        if trigram in trigram_counts:\n",
    "            trigram_counts[trigram] += 1\n",
    "        else:\n",
    "            trigram_counts[trigram] = 1\n",
    "    # Retuns the number of trigrams to trigram_counts\n",
    "    return trigram_counts\n",
    "\n",
    "# Passing the cleaned and combined text to generate trigrams\n",
    "trigram_model = generate_trigrams(cleaned_combined_text)\n",
    "print(\"Trigram model created.\\n\\n Sample trigrams:\")\n",
    "#  Display a sample of the trigrams and their counts\n",
    "print(dict(list(trigram_model.items())[:10]))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1.5\n",
    "Sorting Trigrams\n",
    ">This step is to sort the most frequent trigrams in order to display the most common charecter sequences based on the selected texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most common trigrams:\n",
      "\n",
      "Trigram: ' TH' - Count: 9695\n",
      "Trigram: 'THE' - Count: 7685\n",
      "Trigram: 'HE ' - Count: 5160\n",
      "Trigram: 'ND ' - Count: 3738\n",
      "Trigram: 'AND' - Count: 3507\n",
      "Trigram: ' AN' - Count: 2945\n",
      "Trigram: '   ' - Count: 2924\n",
      "Trigram: 'IS ' - Count: 2562\n",
      "Trigram: ' TO' - Count: 2560\n",
      "Trigram: ' OF' - Count: 2558\n"
     ]
    }
   ],
   "source": [
    "# Initializing sorted_trigrams to store sorted trigrms by frequency in descending order.\n",
    "sorted_trigrams = sorted(trigram_model.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the 10 most common trigrams.\n",
    "print(\"Top 10 most common trigrams:\\n\")\n",
    "for trigram, count in sorted_trigrams[:10]:\n",
    "    print(f\"Trigram: '{trigram}' - Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1.6\n",
    "Saving the Trigram Model\n",
    ">This step is to save the model and data to file to allow easy access to trigram frequencies without having to recompute them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram model has been saved to 'trigram_model.txt'.\n"
     ]
    }
   ],
   "source": [
    "# Save the trigram model to a text file.\n",
    "with open(\"trigram_model.txt\", \"w\") as file:\n",
    "    file.write(\"Trigram Model (Trigram: Count)\\n\")\n",
    "    file.write(\"=\" * 30 + \"\\n\")\n",
    "    for trigram, count in sorted_trigrams:\n",
    "        file.write(f\"{trigram}: {count}\\n\")\n",
    "\n",
    "# Print message of print.\n",
    "print(\"Trigram model has been saved to 'trigram_model.txt'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 Definition \n",
    "**Third-order letter approximation generation**\n",
    "Use your model from Task 1 to generate a string of 10,000 characters. Start with the string TH. Generate each next character by looking at the previous two characters. Find the trigrams in your model that start with those two characters. Randomly select one of the third letters of those trigrams, using the counts as weights.\n",
    "\n",
    "For example, suppose your model has five trigrams starting with TH: THE appeared 150 times, THA appeared 70 times, THI 60 times, TH  50 times, and TH. appeared 10 times. The total of the counts is 340. Select the next character as E with probability 150/340, A with probability 70/340, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "<br>\n",
    " Using the model from Task 1, we will generate the next character based on the  previous. Then choose one of the third letters of the generated trigrams, use the counts as weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the next character based on previous two characters using trigram model\n",
    "def get_next_char(previous_two, trigram_model):\n",
    "    # find trigrams that started with the last two characters\n",
    "    candidates = {k: v for k, v in trigram_model.items() if k.startswith(previous_two)}\n",
    "    \n",
    "    # if no candidates are found, return a space (edge case handling)\n",
    "    if not candidates:\n",
    "        return \" \"\n",
    "    \n",
    "    # extract the possible next characters and their corresponding counts\n",
    "    trigrams = list(candidates.keys())\n",
    "    counts = list(candidates.values())\n",
    "    \n",
    "    # normalize the counts to get probabilities for weighted random selection\n",
    "    total_count = sum(counts)\n",
    "    probabilities = [count / total_count for count in counts]\n",
    "    \n",
    "    # choose a trigram based on the probabilities\n",
    "    chosen_trigram = random.choices(trigrams, weights=probabilities, k=1)[0]\n",
    "    \n",
    "    # return the third character of the chosen trigram    \n",
    "    return chosen_trigram[2]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 \n",
    "Generate 10,000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to generate a string of 10,000 characters\n",
    "def generate_string(trigram_model, length=10000):\n",
    "    result = \"TH\"  # starting with \"TH\" \n",
    "    \n",
    "    # generate characters until the string reaches 10000 characters\n",
    "    for _ in range(length - 2):  # removing two characters as th is counted in this\n",
    "        # return last two characters\n",
    "        last_two = result[-2:]\n",
    "        \n",
    "        # using the trigram model from task 1 predict next character\n",
    "        next_char = get_next_char(last_two, trigram_model)\n",
    "        \n",
    "        # let the next character be the result\n",
    "        result += next_char        \n",
    "    return result\n",
    "\n",
    "# call the method to generate 10,000 characters\n",
    "TH = generate_string(trigram_model, length=10000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T3.1 \n",
    "Firstly we'll load in the words file then store them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45373 words from words.txt.\n"
     ]
    }
   ],
   "source": [
    "# load in the words file\n",
    "def load_words(filepath=\"words.txt\"):\n",
    "    with open(filepath, \"r\") as file:\n",
    "        words = {line.strip().lower() for line in file}\n",
    "    return words\n",
    "\n",
    "# load words into set to store them \n",
    "words_set = load_words()\n",
    "print(f\"Loaded {len(words_set)} words from words.txt.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T3.2\n",
    "We'll then split them up and ensure that is all lowercase for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1682 words.\n"
     ]
    }
   ],
   "source": [
    "# function to split string into words\n",
    "def split_into_words(text):\n",
    "    return text.lower().split()  # make sure all characters are lowercase\n",
    "\n",
    "# split the words from the generated string\n",
    "generated_words = split_into_words(TH)\n",
    "print(f\"Generated {len(generated_words)} words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T3.3\n",
    "Count and return the amound of valid words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid words: 541\n"
     ]
    }
   ],
   "source": [
    "# Count_valid_words takes in our generated string of 10,000 \"TH\" and words_set (words in words.txt file)<br>\n",
    "def count_valid_words(generated_text, words_set):\n",
    "    # A concise for loop iterates through every word in generated_text to compare if its in words_set\n",
    "    valid_words = [word for word in generated_text if word in words_set]\n",
    "    return len(valid_words)\n",
    "\n",
    "# return the count of valid words\n",
    "valid_word_count = count_valid_words(generated_words, words_set)\n",
    "print(f\"Valid words: {valid_word_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T3.4\n",
    "Calculate Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of valid words from words.txt that appear in generated words:\n",
      "======\n",
      "32.16%\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "# calculate the percentage of valid words\n",
    "def calculate_percentage(valid_count, total_count):\n",
    "    if total_count == 0:\n",
    "        return 0\n",
    "    return (valid_count / total_count) * 100\n",
    "\n",
    "\n",
    "total_word_count = len(generated_words)\n",
    "valid_word_percentage = calculate_percentage(valid_word_count, total_word_count)\n",
    "print(f\"Percentage of valid words from words.txt that appear in generated words:\\n======\\n{valid_word_percentage:.2f}%\\n======\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T4.1\n",
    "Save to  JavaScript Object Notation and save in repo as trigrams.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported trigram model to trigrams.json\n"
     ]
    }
   ],
   "source": [
    "# export the trigram model to JSON\n",
    "def export_trigram_model_to_json(trigram_model, filepath=\"trigrams.json\"):\n",
    "    with open(filepath, \"w\") as file:\n",
    "        json.dump(trigram_model, file, indent=4)  # save model as JSON with json formatting, indent as 4 for neatness and readability\n",
    "    print(f\"Exported trigram model to {filepath}\")\n",
    "\n",
    "export_trigram_model_to_json(trigram_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "| Reference     | URL     | Usage | \n",
    "|--------------|-------|-------|\n",
    "| Trigram Research | https://en.wikipedia.org/wiki/Trigram <br> https://web.stanford.edu/~jurafsky/slp3/3.pdf |General|\n",
    "|  Python Naming Convention   | https://peps.python.org/pep-0008/     | General|\n",
    "|  Concise For Method   | https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions     |T3|\n",
    "|  Json Export | https://docs.python.org/3/library/json.html#json.dump <br> https://www.w3schools.com/python/python_json.asp  | T4  |\n",
    "|  Python Random  | https://docs.python.org/3/library/random.html#random.choice  | Imports   |\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
